import pybgpstream
from datetime import datetime
import asyncio
import os
import glob
import numpy as np
from tqdm import tqdm
from sklearn.ensemble import IsolationForest
from bgp_agent import BGPAgent

# --- é…ç½® ---
MAX_CONCURRENT_WORKERS = 5  # å¹¶å‘æ•° (å»ºè®® 3-5ï¼Œé˜²æ­¢ DeepSeek/RIPEstat é™æµ)
QUEUE_SIZE = 10          # é˜Ÿåˆ—ç¼“å†²åŒºå¤§å°

# --- èµ„äº§ç™½åå• ---
ASSET_BASELINE = {
    "104.244.42.0/24": "13414",  # Twitter
    "8.8.8.0/24": "15169",       # Google
    "208.65.153.0/24": "36561"   # YouTube
}

class AnomalyDetector:
    """L2: IForest å¼‚å¸¸æ£€æµ‹å™¨ (ä¿æŒä¸å˜)"""
    def __init__(self):
        self.history = {}
        self.clf = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)
        self.train_data = []
        self.is_fitted = False

    def update_history(self, prefix, origin, as_path):
        if prefix not in self.history:
            self.history[prefix] = {'origins': set(), 'path_lens': []}
        self.history[prefix]['origins'].add(origin)
        self.history[prefix]['path_lens'].append(len(as_path.split()))

    def extract_features(self, prefix, origin, as_path):
        path_len = len(as_path.split())
        if prefix not in self.history:
            return [1, path_len, 0.0]
        record = self.history[prefix]
        is_new_origin = 1 if origin not in record['origins'] else 0
        avg_len = np.mean(record['path_lens'])
        len_diff = abs(path_len - avg_len)
        return [is_new_origin, path_len, len_diff]

    def check(self, prefix, origin, as_path):
        features = self.extract_features(prefix, origin, as_path)
        if not self.is_fitted:
            self.train_data.append(features)
            if len(self.train_data) > 50000:
                self.clf.fit(self.train_data)
                self.is_fitted = True
                tqdm.write("\n[ML] IForest æ¨¡å‹å·²è®­ç»ƒå®Œæ¯•ï¼Œå¼€å§‹ä»‹å…¥...")
            return 1 
        prediction = self.clf.predict([features])[0]
        self.update_history(prefix, origin, as_path)
        return prediction

class BGPStreamPipeline:
    def __init__(self):
        self.agent = BGPAgent()
        self.detector = AnomalyDetector()
        self.queue = asyncio.Queue(maxsize=QUEUE_SIZE)
        print(f"ğŸ¤– ç³»ç»Ÿåˆå§‹åŒ–: å¯ç”¨ {MAX_CONCURRENT_WORKERS} ä¸ªå¹¶å‘ AI ã€‚")

    def _extract_origin(self, as_path):
        if not as_path: return None
        return as_path.split(" ")[-1]

    def _construct_alert_context(self, elem, origin_as):
        return {
            "prefix": elem.fields['prefix'],
            "as_path": elem.fields['as-path'],
            "timestamp": int(elem.time),
            "detected_origin": origin_as,
            "expected_origin": ASSET_BASELINE.get(elem.fields['prefix'], "UNKNOWN"),
        }

    async def worker(self, worker_id):
        """
        æ¶ˆè´¹è€… Worker: ä»é˜Ÿåˆ—å–ä»»åŠ¡ -> è·‘ AI -> å­˜æŠ¥å‘Š
        """
        while True:
            # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ (å¦‚æœæ²¡æœ‰ä»»åŠ¡ä¼šåœ¨è¿™é‡Œç­‰å¾…)
            task_data = await self.queue.get()
            
            alert_context = task_data['context']
            reason = task_data['reason']

            tqdm.write(f"âš¡ [Worker-{worker_id}] å¯åŠ¨è¯Šæ–­: {alert_context['prefix']} ({reason})")

            try:
                # è°ƒç”¨ Agent (è¿™æ˜¯æœ€è€—æ—¶çš„æ­¥éª¤)
                # verbose=False å› ä¸ºå¤šçº¿ç¨‹æ‰“å°ä¼šä¹±ï¼Œæˆ‘ä»¬åªçœ‹ worker çš„æ—¥å¿—
                await self.agent.diagnose(alert_context, verbose=False)
                tqdm.write(f"âœ… [Worker-{worker_id}] è¯Šæ–­å®Œæˆ: {alert_context['prefix']}")
            except Exception as e:
                tqdm.write(f"âŒ [Worker-{worker_id}] ä»»åŠ¡å¤±è´¥: {e}")
            finally:
                # æ ‡è®°è¯¥ä»»åŠ¡å·²å®Œæˆ
                self.queue.task_done()

    async def run_offline_replay(self, file_pattern):
        files = sorted(glob.glob(file_pattern))
        if not files: return

        # 1. å¯åŠ¨å¹¶å‘ Workers
        workers = []
        for i in range(MAX_CONCURRENT_WORKERS):
            w = asyncio.create_task(self.worker(i))
            workers.append(w)

        print(f"\nğŸ“‚ å¼€å§‹å¹¶å‘å¤„ç† {len(files)} ä¸ªæ–‡ä»¶...")

        # 2. ç”Ÿäº§è€…å¾ªç¯ (è¯»å–æ–‡ä»¶)
        for file_path in files:
            abs_path = os.path.abspath(file_path)
            file_name = os.path.basename(file_path)
            tqdm.write(f"\nğŸ“„ [åŠ è½½] {file_name}")
            
            stream = pybgpstream.BGPStream(data_interface="singlefile")
            stream.set_data_interface_option("singlefile", "upd-file", abs_path)
            
            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¯»å–è¿›åº¦
            pbar = tqdm(stream, desc="è¿‡æ»¤æµæ•°æ®", unit="pkt")
            
            for elem in pbar:
                if elem.type != "A": continue
                
                prefix = elem.fields['prefix']
                as_path = elem.fields['as-path']
                origin_as = self._extract_origin(as_path)
                
                # --- è¿‡æ»¤é€»è¾‘ (L1 + L2) ---
                hard_alert = False
                if prefix in ASSET_BASELINE and origin_as != ASSET_BASELINE[prefix]:
                    hard_alert = True
                
                ml_score = 1
                if not hard_alert:
                    ml_score = self.detector.check(prefix, origin_as, as_path)

                # --- è§¦å‘å…¥é˜Ÿ ---
                if hard_alert or (ml_score == -1 and "12389" in as_path):
                    reason = "åŸºå‡†è§„åˆ™æŠ¥è­¦" if hard_alert else "IForestå¼‚å¸¸+å«Œç–‘äºº"
                    
                    alert_context = self._construct_alert_context(elem, origin_as)
                    
                    # æ„é€ ä»»åŠ¡åŒ…
                    task = {
                        'context': alert_context,
                        'reason': reason
                    }
                    
                    # å°†ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ— (å¦‚æœé˜Ÿåˆ—æ»¡äº†ï¼Œè¿™é‡Œä¼šæš‚åœè¯»å–ï¼Œç­‰å¾… Worker æ¶ˆè´¹)
                    # è¿™å°±æ˜¯"èƒŒå‹" (Backpressure) æœºåˆ¶ï¼Œé˜²æ­¢å†…å­˜çˆ†æ‰
                    await self.queue.put(task)
                    
                    tqdm.write(f"ğŸ“¥ [å…¥é˜Ÿ] å‘ç°å¼‚å¸¸ ({reason}) -> é˜Ÿåˆ—é•¿åº¦: {self.queue.qsize()}")

        # 3. ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        tqdm.write("\nâ³ æ–‡ä»¶è¯»å–å®Œæ¯•ï¼Œç­‰å¾… AI Workers å¤„ç†å‰©ä½™ä»»åŠ¡...")
        await self.queue.join() # é˜»å¡ç›´åˆ°é˜Ÿåˆ—æ¸…ç©º
        
        # 4. å–æ¶ˆ Workers
        for w in workers:
            w.cancel()
        
        print("\nğŸ‰ æ‰€æœ‰å¹¶å‘ä»»åŠ¡å·²å®Œæˆï¼")

if __name__ == "__main__":
    pipeline = BGPStreamPipeline()
    local_files = "/home/skypapaya/code/BGP-Anomaly-Trace-Analysis---CotPromote/data/updates*.gz"
    
    loop = asyncio.get_event_loop()
    loop.run_until_complete(pipeline.run_offline_replay(local_files))